---
title: "RegressionModels"
author: "ppar"
date: "8 Dec 2015"
output: 
  html_document: 
    keep_md: yes
---

```{r echo=FALSE}
Sys.setlocale("LC_ALL", "C")
```

#Introduction to Regression - Code Snippet  

##Plotting Marginal Distributions of the __Galton__data. 
The marginal distribution -> children not considering parents and parents not considering children.

```{r warning=FALSE, message=FALSE}
#install.packages("UsingR")
#install.packages("reshape")
library(UsingR)
data(galton)
library(reshape); 

long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable)) 
g <- g + geom_histogram(colour = "black", binwidth=1) 
g <- g + facet_grid(. ~ variable)
g
```

##Finding the empirical mean via least square
Using the following interactive diagram to experiment how the MSE (Mean Squared Error) does change, changing the value of mu. Try to find out the the value of mu that minimize the MSE - What is this value? This value is the same as the sample mean of the data (empirical mean).


The following code snippet must be run in R/RStudio in order to use the manipulate functionality.

```{r eval=FALSE}
library(manipulate)
myHist <- function(mu){
    mse <- mean((galton$child - mu)^2) #Showing the Mean Squared Error- sum of Squared Errors divided by n
    g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth=1)
    g <- g + geom_vline(xintercept = mu, size = 3)
    g <- g + ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
    g
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
```

Playing around the __mu=68__ is the value that minimizes MSE (MSE = 6.34). Teh actual empirical mean is `r mean(galton$child)`. The least squared estimates is the empirical/ sample mean.

```{r collapse=TRUE}
mse <- mean((galton$child - mean(galton$child))^2)
mse
g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth=1)
g <- g + geom_vline(xintercept = mean(galton$child), size = 3)
g
```

##Comparing Children heights vs. Parent heights 
```{r}
ggplot(galton, aes(x = parent, y = child)) + geom_point()
```

One of the main weakness of such diagram is __OVERPLOTTING__. Let´s visualize the data using a different kind of scatterplot where frequence information is showed visually as the size/ color of the plot.

```{r warning=FALSE, message=FALSE}
library(dplyr)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+10, show_guide = TRUE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")                    
g
```

##Regression through the origin 
```{r eval=FALSE}
library(manipulate)
library(dplyr)
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
myPlot <- function(beta){
    g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
    g <- g  + scale_size(range = c(2, 20), guide = "none" )
    g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
    g <- g + geom_point(aes(colour=freq, size = freq))
    g <- g + scale_colour_gradient(low = "lightblue", high="white")                     
    g <- g + geom_abline(intercept = 0, slope = beta, size = 3)
    mse <- mean( (y - beta * x) ^2 )
    g <- g + ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
    g
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
```

Actually for beta = 0.64 the MSE gets its minimum value (circa 5).
 
 We can quickly calculate the slope for the regression through the origin using the following core function. The value found is the same as the previous (experiment).
 
```{r collapse = TRUE}
y <- galton$child
x <- galton$parent
fit <- lm(I(y - mean(y)) ~ I(x - mean(x)))
fit
summary(fit)
```

Let's plot the best fitting line...

```{r }
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")                    
lm1 <- lm(galton$child ~ galton$parent)
g <- g + geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], size = 3, colour = grey(.5))
g
```


#Statistical Regression Models Used for Prediction  

__Homework__ 
__1__ Fit a linear regression model to the father.son dataset: father as predictor & son as response
Give the P-value for the slope coefficient and perform the relevant hypothesis test.

__2__ Interpret both parameters. Recenter for the intercept if necessary.

__3__ Predict the son height if the father height is 80. Would you reccoment this prediction? Why or why not?

```{r collapse=TRUE, message=FALSE, warning=FALSE}
## Part 1
library(UsingR)
library(ggplot2)
data(father.son)

g <- ggplot(father.son, aes(y = sheight, x = fheight))
g <- g + xlab("Father Height")
g <- g + ylab("Son Height")
g <- g + geom_point(size = 5, color = "blue", alpha = 0.2)
g <- g + geom_smooth(method = "lm", color = "black")
g

#Calculate parameters of the linear regression model manually
x <- father.son$fheight
y <- father.son$sheight
beta1 <- cor(y,x) * (sd(y)/ sd(x))
beta0 <- mean(y) - beta1 * mean(x)
beta0
beta1

#Fit a linear regression model to the father.son dataset
#father as predictor & son as response
fit <- lm(y ~ x)
summary(fit)$coefficients

#Lets look at the beta1 coefficients
#Remember -> response = beta0 + beta1 * predictor
#                    Estimate Std. Error  t value     Pr(>|t|)
#(Intercept)        33.886604 1.83235382 18.49348 1.604044e-66
#father.son$fheight  0.514093 0.02704874 19.00618 1.121268e-69 (****)
#                                                    (**)
# The line (****) give information about the following hypothesis testing
# H0: beta1 = 0 and Ha: beta1 != 0
# (**) we have a t value of almost 20 and an p-valuo of 10^69.
# This imply that we are going to reject the null hypothesis (H0) and accept Ha
# so there is a linear relationship between the son and father heights


## Part 2
#Intercept: expected value of the response when the predictor is 0 (not very meaningful)
#slope: increase of the resposponse for a unit increase in the predictor

#Shifting can be applied to provide meaning to the Intercept
x <- father.son$fheight - mean(father.son$fheight)
y <- father.son$sheight

g <- ggplot(data.frame(x, y), aes(y = y, x = x))
g <- g + xlab("Father Height - mean")
g <- g + ylab("Son Height")
g <- g + geom_point(size = 5, color = "blue", alpha = 0.2)
g <- g + geom_smooth(method = "lm", color = "black")
g

fit2 <- lm(y ~ x)
summary(fit2)$coefficients

#Intercept = 68.68 - the expected value of the response (son height) when 
#predictor is as the mean of father heights. Nope!! Slope unchanged

##Part 3
z <- 80
predict(fit, newdata = data.frame(x = z))
#Estimated heigh is circa 75
g <- ggplot(father.son, aes(y = sheight, x = fheight))
g <- g + xlab("Father Height")
g <- g + ylab("Son Height")
g <- g + geom_point(size = 5, color = "blue", alpha = 0.2)
g <- g + geom_smooth(method = "lm", color = "black")
g <- g + geom_vline(xintercept=75.01405, color = "red", size = 1)
g
#Looking at the plot we can see a high variation and few data available for that specific estimate
#we would not trust much such estimate (look also at the summary of the father data and you will see
#that we are operating at the edge of the available data for our model..
```

__4__ Load the mtcarsdataset. Fit a linear regression model with miles per gallon as response and horsepower as predictor.
Interpret the coefficients and recenter intercept if necessary.

```{r collapse = TRUE}
data("mtcars")

g <- ggplot(mtcars, aes(y = mpg, x = hp))
g <- g + xlab("Horse Power")
g <- g + ylab("Miles Per Gallon")
g <- g + geom_point(size = 5, color = "blue", alpha = 0.2)
g <- g + geom_smooth(method = "lm", color = "black")
g

fit <- lm(mpg ~ hp, data = mtcars)
summary(fit)$coefficients

#               Estimate Std. Error   t value     Pr(>|t|)
#(Intercept) 30.09886054  1.6339210 18.421246 6.642736e-18
#hp          -0.06822828  0.0101193 -6.742389 1.787835e-07

#Intercept -> expected response value when predictor value is 0 (not meaningful)
#Shifting is necessary in order to give meaning to the intercept

fit <- lm(mpg ~ I(hp - mean(hp)), data = mtcars)
summary(fit)$coefficients

#                    Estimate Std. Error   t value     Pr(>|t|)
#(Intercept)      20.09062500  0.6828817 29.420360 1.101810e-23
#I(hp - mean(hp)) -0.06822828  0.0101193 -6.742389 1.787835e-07

#Now Intercept is the expected response value when predictor has average value of horse power car
#Slope (not changed with the shifting): increase/decrease of the response for a unit increase in the predictor.

#Is there a linear model between x and y?
#slope          -0.06822828  0.0101193 -6.742389 1.787835e-07
#Ho: beta1 = slope = 0, Ha: beta1 = slope != 0
#t statistic is -6.7 and the P-Value is 10^-7 pretty slow so we can reject the null hypothesis

```

#Residuals

__Homework__ 
__1__ Fit a linear regression model to the father.son dataset with the father as the predictor and the son as the outcome. Plot the son’s height (horizontal axis) versus the residuals (vertical axis).

__2__ Directly estimate the residual variance and compare this estimate to the output of lm.

__3__ Give the R squared for this model.


```{r collapse=TRUE, warning=FALSE, message=FALSE}
library(UsingR)
data("father.son")
#x: predictor, y: response
x <- father.son$fheight
y <- father.son$sheight
n <- length(x)

##Part 1
fit <- lm(y ~ x)
summary(fit)
res <- resid(fit)

#Plor residuals vs predictor

plot(x, res,  
     xlab = "Son Height", 
     ylab = "Residuals (height)", 
     bg = "lightblue", 
     col = "black", cex = 2, pch = 21,frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(res[i], 0), col = "red" , lwd = 2)


##Note the possibility to use plot(fit)
#It will plot interactively different and relevant plots
#plot(fit)

##Part 2
#Residual variance
(1 / (n - 2)) * sum(res^2)
summary(fit)$sigma^2

##Part 3
ycappello <- predict(fit)
a_num <- sum((ycappello - mean(y))^2)
a_den <- sum((y - mean(y))^2)

rSquared <- a_num/ a_den
rSquared #25%

summary(fit)$r.squared
```

__4__ Load the mtcars dataset. Fit a linear regression with miles per gallon as the outcome and
horsepower as the predictor. Plot horsepower versus the residuals. 

__5__ Directly estimate the residual variance and compare this estimate to the output of lm.

__6__ Give the R squared for this model.

```{r collapse=TRUE, warning=FALSE, message=FALSE}
##Part4
data("mtcars")
response <- mtcars$mpg
predictor1 <- mtcars$hp
n <- length(response)

fit <- lm(response ~ predictor1)
a_residuals <- resid(fit)

plot(predictor1, a_residuals,  
     xlab = "Horse Power", 
     ylab = "Residuals (mpg)", 
     bg = "lightblue", 
     col = "black", cex = 2, pch = 21,frame = FALSE)

for (i in 1 : n) 
  lines(c(predictor1[i], predictor1[i]), c(a_residuals[i], 0), col = "red" , lwd = 1)


##Part 5
summary(fit)$sigma^2
(1/(n-2)) * sum(a_residuals^2)

##Part 6
responseCappello <- predict(fit)
a_num = sum((responseCappello - mean(response))^2)
a_den <- sum((response - mean(response))^2)
a_num / a_den #R squared
summary(fit)$r.squared
```

#Regression Inference  
__Part 1__  
1. Test whether the slope coefficient for the father.son data is different from zero (father as predictor, son as outcome).  
2. Refer to question 1. Form a confidence interval for the slope coefficient.  
3. Refer to question 1. Form a confidence interval for the intercept (center the fathers’ heights first to get an intercept that is easier to interpret).  
4. Refer to question 1. Form a mean value interval for the expected son’s height at the average father’s height.  
5. Refer to question 1. Form a prediction interval for the son’s height at the average father’s height.  

```{r collapse=TRUE, message=FALSE, warning=FALSE}
rm(list = ls())
library(UsingR)
data("father.son")
x <- father.son$fheight; y <- father.son$sheight
fit <- lm(y ~ x)
summary(fit)$coefficients
```
1. 
             Estimate Std. Error  t value     Pr(>|t|)
(Intercept) 33.886604 1.83235382 18.49348 1.604044e-66
x            0.514093 0.02704874 19.00618 1.121268e-69

Slope -> Hypothesis Testing H0: slope = 0, Ha: slope != 0
t-statistic is pretty significant under the Null hypothesis and we can see that the probability of getting a result as extreme as the one  we got is around 10^-69 (zero)... so we can reject the null hypothesis under such conditions.
There is a linera relationship between the response and the predictor.
```{r collapse=TRUE}
#2. Confidence interval for the slope
e_beta1 <- summary(fit)$coefficients[2,1]
se_beta1 <- summary(fit)$coefficients[2,2]

e_beta1 + c(-1, 1) * qt(0.975, fit$df) * se_beta1
#95% confidence interval  ....

#Another option is to use the confint function
confint(fit)
```

```{r collapse=TRUE}
#3. Confidence interval for the intercept
#The intercept is the expected value of the response when the predictor is 0 - not very meaningful in this condition
#Shifting the predictor

x_shifted <- father.son$fheight - mean(father.son$fheight)
fit2 <- lm(y ~ x_shifted)
summary(fit2)$coefficients
confint(fit2)

#Calculating it from scratch
beta0_e <- summary(fit2)$coefficients[1,1]
beta0_se <- summary(fit2)$coefficients[1,2]

beta0_e + c(-1,1) * qt(0.975, df=fit2$df) * beta0_se

```

```{r collapse=TRUE}
#4. & 5. Mean Interval and Prediction interval
library(ggplot2)
x <- father.son$fheight; y <- father.son$sheight
newx <- data.frame(x = seq(min(x), max(x), length = 100))
p1 <- data.frame(predict(fit, newdata= newx,interval = ("confidence")))
p2 <- data.frame(predict(fit, newdata = newx,interval = ("prediction")))
p1$interval <- "confidence"
p2$interval <- "prediction"
p1$x <- newx$x
p2$x <- newx$x
dat <- rbind(p1, p2)
names(dat)[1] <- "y"

g <- ggplot(dat, aes(x = x, y = y))
g <- g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2) 
g <- g + geom_line()
g <- g + geom_vline(xintercept = mean(x), color = "red")
g <- g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)
g

fatherHeigth_m <- data.frame(x = c(mean(x)))
predict(fit, newdata= fatherHeigth_m,interval = ("confidence"))
predict(fit, newdata = fatherHeigth_m,interval = ("prediction"))

```

#Multivariable Regression Analysis
##Simulation
```{r }
n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
#Generate the y
#adding an intercept and a gaussian noise
y <- 10 + x1 + x2 + x3 + rnorm(n, sd = .1)

#From y, x1, x2, x3 - let's estimate the coefficient model (linear regression model)
fit <- lm(y ~ x1 + x2 + x3)
summary(fit)

#Coefficients:
#            Estimate Std. Error t value Pr(>|t|)    
#(Intercept) 10.01663    0.01089  920.02   <2e-16 ***
#x1           1.00083    0.01111   90.05   <2e-16 ***
#x2           0.99570    0.01221   81.55   <2e-16 ***
#x3           0.98936    0.01081   91.54   <2e-16 ***

#Let's try to calculate the coefficient using understandin behind
#how multivariable regression model works

#Estimate beta1 (x1)
#Get the residual where the response and predictor(x1) having moved out all of the
#others predictors (x2, x3) (same for y)

e_x1 <- resid(lm(x1 ~ x2 + x3))
e_y <- resid(lm(y ~ x2 + x3))

sum(e_x1 * e_y) / sum(e_x1^2)
summary(lm(e_y ~ e_x1 - 1))$coefficients
```
__Homework 1__  
1. Load datase Seatbelts and fit a linear mode of driver deaths (response) with kms and petrol as predictors. 

2. And predict the number of death at the average kms and petrol price.

3. Take the residuals fro DriversKilled having regressed out kms and an intercept and the residual fro petrol having regressed out kms and an intercept. Fit a regression through the origin of the two residuals and show that it is the same as the coefficient in step 1.

```{r}
library(datasets)
data("Seatbelts")

#Setabealts is a time series object so we need to transform into a dataframe
seatbelts <- as.data.frame(Seatbelts)
head(seatbelts)

fit <- lm(DriversKilled ~ kms + PetrolPrice, data = seatbelts)
summary(fit)$coefficients
round(summary(fit)$coef,4)

#Looking at the coefficients (estimated) we can see that they have different magnitudes
#this make quite difficult to interpret the resul. Moreover if we look at the meaning of
#the intercept expected no of drivers killed when predictors are set to 0 
# (0 kms, 0 Petrol Price) (intercept is not very meaningful)

#What we can do is to center the kms and petrol price and maybe rescaling them to a more meaningful. For example looking at the data kms and Petrol Price

summary(seatbelts$kms)
#kms in the magnitude of thousand so 1km change is not actually really meaningful

summary(seatbelts$PetrolPrice)
#Petrol Price is in the magnitude 10^-1

#Lets do some change to our data structure and create more meaningful data for the regression
library(dplyr)
seatbelts <- mutate(seatbelts,
                   pp = (PetrolPrice - mean(PetrolPrice)) / sd(PetrolPrice),
                   mm = kms / 1000,
                   mmc = mm - mean(mm))
head(seatbelts)

fit <- lm(DriversKilled ~ mmc + pp, data = seatbelts)
summary(fit)
#Now the data seems more reasonable and meaningful (holding other predictors constant)
# mmc 1.7 decrease in death for a 1 increment (1000km) from the mean
# pp 7.83 decrease in death for a 1 standard deviation change in pp  

#Predict number of death for the average kms and PetrolPrice
seatbelts <- as.data.frame(Seatbelts)
y <- seatbelts$DriversKilled
x1 <- seatbelts$kms
x2 <- seatbelts$PetrolPrice

fit <- lm(y ~ x1 + x2)
predict(fit, newdata = data.frame(x1 = mean(x1), x2 = mean(x2)))

#.3
seatbelts <- as.data.frame(Seatbelts)
dk <- seatbelts$DriversKilled
kms <- seatbelts$kms
pp <- seatbelts$PetrolPrice

fitfull <- lm(dk ~ kms + pp)

#Regreess kms out of dk including the intercept
edk <- resid(lm(dk ~ kms))
#Regreess kms out of pp including the intercept
epp <- resid(lm(pp ~ kms))
summary(lm(edk ~ epp - 1)) #no intercept, through the origin

summary(fitfull)$coef
#estimate for the pp is identical usingteh two different way to calculate it
```
#Multivariate Examples & Tricks

```{r collapse=TRUE}
require(datasets)
require(GGally)
require(ggplot2)

data("swiss")
#?swiss
g <- ggpairs(swiss, lower = list(continuous = "smooth"), params = c(method = "loess"))
g

#lets see the result of calling lm on this datase
summary(lm(Fertility ~ ., data = swiss))
summary(lm(Fertility ~ ., data = swiss))$coef

#Unudjusted estimates
summary(lm(Fertility ~ Agriculture, data = swiss))

summary(lm(Fertility ~ Agriculture + Education + Examination, data = swiss))

```

__Homework 1__  
1. Load datase Seatbelts and fit a linear mode of driver deaths (response) with kms and petrol as predictors.

2. Repeat question 1 for the outcome being the log of the count of driver deaths. Interpret your coefficients.

3. Add the dummy variable law and interpret the results. Repeat this question
with a factor variable that you create called lawFactor that takes the levels No and Yes. Change the reference level from No to Yes.

4. Discretize the PetrolPrice variable into four factor levels. Fit the linear model with this factor to see how R treats multiple level factor variables

```{r collapse=TRUE}
library(datasets)
data("Seatbelts")

#Setabealts is a time series object so we need to transform into a dataframe
seatbelts <- as.data.frame(Seatbelts)
head(seatbelts)

fit <- lm(DriversKilled ~ kms + PetrolPrice, data = seatbelts)
summary(fit)$coefficients
round(summary(fit)$coef,4)
```

```{r collapse=TRUE}
library(datasets)
data("Seatbelts")
seatbelts <- as.data.frame(Seatbelts)
#2.
#logging of the outcome
library(dplyr)
#normalized in order to have a better scale
seatbelts <- mutate(seatbelts,
                   pp = (PetrolPrice - mean(PetrolPrice)) / sd(PetrolPrice),
                   mm = kms / 1000,
                   mmc = mm - mean(mm))
head(seatbelts)

fit <- lm(I(log(DriversKilled)) ~ mmc + pp, data = seatbelts)
summary(fit)$coefficients

#All of the estimates are now interpreted in the log scale. We can then
exp( -0.06412578)
1 - exp( -0.06412578)
#circa 6% decrease in geometric mean death for a 1 sd increase in pp holding mmc constant. 
#3.
fit <- lm(DriversKilled ~ mmc + pp + law, data = seatbelts)
summary(fit)
#The law variable is an factor variable and this doesc change our intercept.
#124 (intercept) is the expected number of death for the average ppn, the average km and before the law was in effect (law zero).
#If we want to know the intercept when the law was in effect we need to add -11.889 to the previous value. 11.889 fewer death were because of the law being in effect.
#Other variable are interpreted as before (with law variable constant (zero or one)
```

```{r collapse=TRUE}
#4.
library(datasets)
data("Seatbelts")
seatbelts <- as.data.frame(Seatbelts)
seatbelts <- mutate(seatbelts,
                   pp = (PetrolPrice - mean(PetrolPrice)) / sd(PetrolPrice),
                   ppf = as.factor((pp <=-1.5)+(pp<=0)+(pp<=1.5)+(pp<Inf)),
                   mm = kms / 1000,
                   mmc = mm - mean(mm))
head(seatbelts)
table(seatbelts$ppf)

fit <- lm(DriversKilled ~ mmc + ppf + law, data = seatbelts)
summary(fit)

#ppf1 is used (choosen) as a reference value - while the other are used explicitly in the model. Each coefficient estimate (ppf2, ppf3, ppf4) is interpreted as a comparison to the reference level.
```

```{r echo=FALSE, eval = FALSE}
#1. Consider the mtcars data set. Fit a model with mpg as the outcome that includes number of cylinders as a factor variable and weight as confounder. Give the adjusted estimate for the expected change in mpg comparing 8 cylinders to 4.
rm(list = ls())
data("mtcars")
str(mtcars)
y <- mtcars$mpg
x1 <- as.factor(mtcars$cyl)
x2 <- mtcars$wt
table(x1)

fit0 <- lm(y ~ x1 + x2) #adjusted model
summary(fit0)

data_a <- data.frame(y = y, x1  = x1, x2 = x2)
fit1 <- lm(y ~ x1 + x2, data = data_a) #adjusted model
summary(fit1)

data_b <- relevel(data_a$x1, ref = "4")
fit2 <- lm(y ~ x1 + x2, data = data_b) #adjusted model
summary(fit2)

#expected change in mpg when comparing 8cyl to 4cyl (reference) is -6.0709

#2. Consider the mtcars data set. Fit a model with mpg as the outcome that includes number of cylinders as a factor variable and weight as a possible confounding variable. Compare the effect of 8 versus 4 cylinders on mpg for the adjusted and unadjusted by weight models. Here, adjusted means including the weight variable as a term in the regression model and unadjusted means the model without weight included. What can be said about the effect comparing 8 and 4 cylinders after looking at models with and without weight included?

rm(list = ls())
data("mtcars")
str(mtcars)
y <- mtcars$mpg; x1 <- as.factor(mtcars$cyl); x2 <- mtcars$wt
data_a <- data.frame(y = y, x1  = x1, x2 = x2)
data_b <- relevel(data_a$x1, ref = "4")
fit_notAdj <- lm(y ~ x1)
round(summary(fit_notAdj)$coeff, 3)
#Not adjusted effect 8cyl (versus 4cyl ref): -11.564
fit_adj <- lm(y ~ x1 + x2)
round(summary(fit_adj)$coeff, 3)
#Adjusted effect 8cyl (versus 4cyl ref): -6.071 holding the other regressor constant
#Conclusion: Holding weight constant, cylinder appears to have less of an impact on mpg than if weight is disregarded.

#3. Consider the mtcars data set. Fit a model with mpg as the outcome that considers number of cylinders as a factor variable and weight as confounder. Now fit a second model with mpg as the outcome model that considers the interaction between number of cylinders (as a factor variable) and weight. Give the P-value for the likelihood ratio test comparing the two models and suggest a model using 0.05 as a type I error rate significance benchmark.

rm(list = ls())
data("mtcars")
y <- mtcars$mpg; x1 <- as.factor(mtcars$cyl); x2 <- mtcars$wt

fit1 <- lm(y ~ x1 + x2)
round(summary(fit1)$coef,3)

fit2 <- lm(y ~ x1 * x2) #Interaction between cylinder and weight
round(summary(fit2)$coeff,3)

#The P-value is larger than 0.05. So, according to our criterion, we would fail to reject, which suggests that the interaction terms may not be necessary.

#4. Consider the mtcars data set. Fit a model with mpg as the outcome that includes number of cylinders as a factor variable and weight inlcuded in the model as lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars). How is the wt coefficient interpretted?

rm(list = ls())
data("mtcars")

fit <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
round(summary(fit)$coef,3)
#The estimated expected change in MPG per one ton increase in weight.

#5. Give the hat diagonal for the most influential point
rm(list = ls())
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)

fit <- lm(y ~ x)
max(hatvalues(fit))
#0.9946

#Give the slope dfbeta for the point with the highest hat value.
round(dfbetas(fit),0)
#-134

#7. It is possible for the coefficient to reverse sign after adjustment. For example, it can be strongly significant and positive before adjustment and strongly significant and negative after adjustment.
```
